# -*- coding: utf-8 -*-
"""Book데이터를 이용한 컨텐츠 기반 추천시스템.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j7g5kPbwqT7BnpOVkn4to91aHfWaUTTu
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install kss

import os
import kss
import re
import pandas as pd
import numpy as np
from tqdm import tqdm  # 작업량 확인하는 기능

# 형태소 기반 토크나이징 (Konlpy)
!python3 -m pip install konlpy
!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

from konlpy.tag import Mecab
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
mecab = Mecab()

path = '/content/drive/MyDrive/data/books/'
review_df = pd.read_csv(path + 'reviewdata_limit.csv', encoding='CP949', error_bad_lines=False)
review_words = []
for w1 in review_df['Column7'].values[1:]:
  tmp_list = mecab.nouns(w1)
  for w2 in tmp_list:
    review_words.append(w2)
# print(review_words)

total_review_cnt = len( review_df['Column7'].values[1:])
review_count_dict = dict.fromkeys(list(set(review_words))) # 중복을 제거한 단어를 key로 두고 value에는 해당단어 개수.

for word in review_words: # 전체 단어를 살피며 해당 단어 개수를 카운트
  if review_count_dict[word] == None:
    review_count_dict[word] = 1
  else:
    review_count_dict[word] += 1
print(review_count_dict)

review_idf = dict()
for each_review in review_count_dict:
  review_idf[each_review] = np.log10(total_review_cnt / review_count_dict[each_review]) 

print(review_idf)

review_representation = pd.DataFrame(columns=sorted(list(set(review_words))), index=list(set(review_df['Column2'][1:])))
for idx, each_row in tqdm(review_df.groupby(by='Column2')): # idx는 book_id, Column2는 book_id를 의미함
  temp_list = list(each_row['Column7'])
  dict_temp = dict()
  for sentence in temp_list:
    temp_noun = list(set(mecab.nouns(sentence)))
    for word in temp_noun:
      dict_temp[word] = review_idf[word]
  row_to_add = pd.DataFrame([dict_temp], index=[idx])
  review_representation.update(row_to_add)

review_representation = review_representation.fillna(0)
print(review_representation)

# Cosine similarity사용하여 유사도 평가
from sklearn.metrics.pairwise import cosine_similarity

def cos_sim_matrix(a, b):
    cos_sim = cosine_similarity(a, b)
    result_df = pd.DataFrame(data=cos_sim, index=[a.index])
    return result_df

# 행(index): book_id, 열(columns): book_id, 열도 0번부터 94764887이다
# 행 번호94764887과 열번호 0은 같은 도서이므로 유사도가 1이 나옴 // 같은 대상은 유사도가 1
# 유사도 범위 -1 <=  <= 1  // -1:완전 다름, 1:같음
cs_df = cos_sim_matrix(review_representation, review_representation)
cs_df.columns=list(set(review_df['Column2'][1:])) # column이름 변경

cs_df

print(cs_df.shape)
print(cs_df["94764887"].sort_values(ascending=False).head())

"""# **Scikit-learn을 사용한 TF-IDF**
  - 한글자 단어 제외

"""

from collections import defaultdict

print(len(all_sentences))
print(all_sentences[0], '\n')
print(all_sentences[1], '\n')

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(all_sentences)

word2id = defaultdict(lambda : 0)
for idx, feature in enumerate(tfidf_vectorizer.get_feature_names()):
    word2id[feature] = idx

tfidf_vectorizer.vocabulary_

tfidf_matrix.shape

with open(os.path.join(data_path, 'news_sample.txt'), 'r', encoding='utf-8') as f:
  for idx, line in enumerate(f.readlines()):
    print(f"--- 문서 {idx} 번 ---")
    print(line[:200])

